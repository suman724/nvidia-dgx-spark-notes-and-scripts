## DGX Spark (single node) — K3s + NVIDIA GPU support runbook

This document gives details on how to install, troubleshoot and test the installation of Kubernetes using K3s on NVIDIA DGX Spark on a Single node.

---

# 0) Preconditions

### A) Host GPU + driver works

```bash
nvidia-smi
```

Expected: GPU listed (GB10), driver/CUDA versions shown.

### B) NVIDIA container tools are present

```bash
which nvidia-container-runtime
which nvidia-ctk
```

Expected: both commands print paths (e.g., `/usr/bin/...`).

---



# 1) Install K3s

### Install K3s (kubeconfig permissions via group + mode)

```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-group k3s --write-kubeconfig-mode 640" sh -
```

### Ensure group exists and your user can read kubeconfig (recommended)

```bash
sudo groupadd -f k3s
sudo usermod -aG k3s $USER
# log out/in or open a new SSH session so group membership applies
```

**Quick workaround (less secure):**

```bash
sudo chmod 644 /etc/rancher/k3s/k3s.yaml
```

### Verify K3s is up

```bash
kubectl get nodes -o wide
kubectl get pods -n kube-system
```

Expected: node `Ready`; system pods running.

---

# 2) Configure NVIDIA Container Toolkit (host-level) + refresh K3s containerd config

> K3s generates its embedded containerd config at: `/var/lib/rancher/k3s/agent/etc/containerd/config.toml`
>
> You do not edit this file directly. Instead, ensure the NVIDIA runtime is installed/configured on the host, then restart K3s so it regenerates the file and picks up the NVIDIA runtime handler.

### A) Configure NVIDIA container toolkit for containerd (host step)

```bash
sudo nvidia-ctk runtime configure --runtime=containerd
```

### B) Restart K3s (regenerates K3s containerd config and detects NVIDIA runtime)

```bash
sudo systemctl restart k3s
```

### C) Verify K3s containerd config includes the NVIDIA runtime handler

```bash
sudo grep -n "runtimes.'nvidia'|BinaryName" /var/lib/rancher/k3s/agent/etc/containerd/config.toml
```

Expected: shows `runtimes.'nvidia'` and `BinaryName = "/usr/bin/nvidia-container-runtime"`.

---

# 3) Verify Kubernetes RuntimeClass exists

K3s often creates runtime classes automatically; confirm `nvidia` exists:

```bash
kubectl get runtimeclass
kubectl get runtimeclass nvidia -o yaml
```

Expected:

- `name: nvidia`
- `handler: nvidia`

If missing, create it:

```bash
cat <<'EOF' | kubectl apply -f -
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF
```

---

# 4) Install NVIDIA device plugin (Spark-compatible)

### A) Install baseline device plugin manifest

```bash
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.1/deployments/static/nvidia-device-plugin.yml
```

### B) Upgrade to a Spark-compatible version

DGX Spark/GB10 can return `Not Supported` for NVML memory queries; use a newer plugin that **ignores** that error.

```bash
kubectl -n kube-system set image ds/nvidia-device-plugin-daemonset \
  nvidia-device-plugin-ctr=nvcr.io/nvidia/k8s-device-plugin:v0.18.1
```

### C) Force NVML discovery strategy

```bash
kubectl -n kube-system set env ds/nvidia-device-plugin-daemonset DEVICE_DISCOVERY_STRATEGY=nvml
```

### D) Ensure device plugin pods run using NVIDIA runtime

```bash
kubectl -n kube-system patch ds nvidia-device-plugin-daemonset --type='json' \
  -p='[{"op":"add","path":"/spec/template/spec/runtimeClassName","value":"nvidia"}]'
```

### E) Restart rollout and verify

```bash
kubectl -n kube-system rollout restart ds/nvidia-device-plugin-daemonset
kubectl -n kube-system rollout status ds/nvidia-device-plugin-daemonset
kubectl -n kube-system get pods | grep -i nvidia
```

### F) Verify plugin logs (success signals)

```bash
kubectl -n kube-system logs $(kubectl -n kube-system get pods | awk '/nvidia-device-plugin-daemonset/{print $1; exit}') --tail=200
```

Expected log signals:

- `deviceDiscoveryStrategy: "nvml"`
- `Ignoring error getting device memory: Not Supported`
- `Registered device plugin for 'nvidia.com/gpu' with Kubelet`

---

# 5) Verify GPU is visible to the node

```bash
kubectl get node saispark -o jsonpath='{.status.capacity.nvidia\.com/gpu}{"\n"}'
kubectl get node saispark -o jsonpath='{.status.allocatable.nvidia\.com/gpu}{"\n"}'
```

Expected:

- `1`
- `1`

---

# 6) End-to-end GPU smoke test

```bash
cat <<'EOF' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: gpu-smoke
spec:
  runtimeClassName: nvidia
  restartPolicy: Never
  containers:
  - name: cuda
    image: nvcr.io/nvidia/cuda:13.1.0-devel-ubuntu24.04
    command: ["bash","-lc","nvidia-smi && echo OK"]
    resources:
      limits:
        nvidia.com/gpu: 1
EOF

kubectl get pod gpu-smoke -o wide
kubectl logs gpu-smoke
kubectl delete pod gpu-smoke
```

Expected: `nvidia-smi` output + `OK`

---

# Reference: NVIDIA runtime stanza in the generated K3s containerd config

> For recognition only — do not hand-edit `config.toml`.

```bash
sudo grep -n "runtimes.'nvidia'|BinaryName" /var/lib/rancher/k3s/agent/etc/containerd/config.toml
```

You should see something equivalent to:

```toml
[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.'nvidia']
  runtime_type = "io.containerd.runc.v2"

[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.'nvidia'.options]
  BinaryName = "/usr/bin/nvidia-container-runtime"
  SystemdCgroup = true
```

---

## Quick troubleshooting cues

- **No **``** on node** → device plugin not registering.

  - Confirm DS image is `v0.18.1`
  - Confirm `DEVICE_DISCOVERY_STRATEGY=nvml`
  - Confirm DS has `runtimeClassName: nvidia`
  - Check plugin logs for registration with kubelet

- **K3s won’t start after template edits** → remove/backup custom containerd templates and restart K3s.

- \*\*Device plugin fails with \*\*`` → upgrade plugin to `v0.18.1+` and keep NVML strategy.



